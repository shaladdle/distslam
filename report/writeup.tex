\documentclass[prodmode,acmtecs]{acmsmall} % Aptara syntax

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[margin=1in]{geometry}
\usepackage{exscale}
\usepackage[]{graphicx}
\usepackage[]{graphics}
\usepackage{listings}
\usepackage{float}

% Package to generate and customize Algorithm as per ACM style
\usepackage[ruled]{algorithm2e}
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}

% no indentation - just spaces between paragraphs
\usepackage[parfill]{parskip}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{tocdepth}{2}

\newcommand{\qt}[1]{``#1''}
\newcommand{\code}[1]{\lstinline{#1}}

\lstset{
    language=[ANSI]C,
    basicstyle=\ttfamily,
    tabsize=4
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{
    CMU 15\-780 Graduate Artificial Intelligence
    Final Project: Distributed SLAM
}

\author{
    Adam Wright, Tim Kuehn, \& Nathan Slobody \\
}

\begin{abstract}

We attempt a simple method of combining the results of multiple robots conducting SLAM in an unknown environment with known landmarks.

\end{abstract}

\date{\today}

\begin{document}

\maketitle

\setcounter{tocdepth}{2}

\section{Introduction}

Localization and mapping are fundamental problems in the area of mobile robotics.  For a robot to accomplish a task within an environment, it must have knowledge of the layout of the environment, in the form of a map showing where its goals and obstacles lie.  However, many robotics scenarios involve a robot entering an unknown environment without a map.  Before executing search and planning techniques, it must therefore create a map of the environment in order to find its way through and hope to accomplish any goals.  

However, even when the robot has a map, it will not be useful if it is unable to localize itself on that map.  A robot's internal state estimation is thus equally crucial to task completion.

Many real-world applications of robotics, such as search and rescue or indoor fire fighting, do not have the luxury of having an accurate map beforehand. Simultaneous Localization and Mapping (SLAM) techniques, which build maps based only on sensor measurements, allow autonomous robots in these applications to succeed. Further, some applications would benefit from using multiple robots to cover a wide area quickly.

In this project, we implement a simple system for distributed SLAM using multiple robots in simulation.  These robots perform SLAM individually and share their data to create a larger combined map of the area.

\section{Background}

\subsection{SLAM}

The idea behind SLAM is to use a probabilistic method to solve mapping and localization as a single problem.  Mapping and localization are typically both attempted under conditions of sensor noise and uncertain state estimation.  SLAM techniques take advantage of the idea that feedback from both processes can reinforce each other at each step of an iterative state estimation; in fact, SLAM's model states that it is impossible in practice to separate the two.

Since, in theory, accurate localization can not be achieved without an accurate map on which to localize, and an undistorted map cannot be created without an accurate statement of present localization, the SLAM problem is iterative in practice.  From some starting conditions, better estimates of both are converged towards over time and subsequent observations.

SLAM is an active research topic with several current approaches, including various varieties of Kalman filtering (linear, extended, unscented), particle filtering, smoothing, and others.  The implementation we chose is based on a linear Kalman filter.

\subsection{Kalman Filter}

The Kalman filter is a method of updating prior predictions based on current observations.  At a high level it involves a transition model describing the relationship between a state at two consecutive timesteps $x_t$ and $x_{t+1}$, and a measurement model describing the relationship between what is perceived and the system state at a given timestep.

The transition model is given by:
\begin{align*}
    x_t' &= F x_t + G u_t + \nu_t \\
\end{align*} 

where $\nu_t$ is gaussian noise with covariance matrix $Q$.

The measurement model is given by:

\begin{align*}
    Z_k &= H_k x_k + w_k
\end{align*}

where $w_k$ is gaussian noise with covariance matrix $R$.

The Kalman filter involves two steps: Predict and Update. \\

Predict:

\begin{align*}
    x_t' &= F x_t + G u_t \\
    P_t' &= F P F^T + Q
\end{align*}

Update:

\begin{align*}
    y &= z_t - H_t x_t' \\
    S &= H_t P_t' H^T_t + R_t \\
    K &= P_t' H^T_t S^{-1} \\
    x_{t+1} &= x_t' + Ky \\
    P_{t+1} &= (I - K H_t) P_t'
\end{align*}

The matrix definitions are as follows:

\begin{align*}
    H:& \text{ Motion model}\\
    P:& \text{ Covariance matrix}\\
    R:& \text{ Measurement uncertainty}\\
    x:& \text{ System state}\\
    Q:& \text{ Motion covariance / uncertainty}
\end{align*}

\section{Model}

The problem that our model addresses is that of a robot entering an unknown area but searching for known landmarks within it.  However, while it is able to recognize the landmarks upon sensing them, it still does not know what their absolute positions are \- only the positions relative to itself and what it has already mapped.

For analogy, consider a person walking into the Louvre museum without a map.  He does not know much about art and doesn't know where any paintings are within the museum.  However, he knows what the famous ones look like and when he sees the Mona Lisa he recognizes it immediately. If he has been tracing his steps from the beginning, can give a position for the painting relative to the museum's entry point.  If he later comes across the Venus de Milo, he can relate its position to both the Mona Lisa and his entry point.  (Of course, if he daydreamed while walking to the Mona Lisa and wasn't sure about how many steps he actually took, all of the relative positions will be slightly off.)

This is similar to the model we are exploring here, with the Louvre standing in for an unknown area, the famous artworks being landmarks, and the robot daydreaming representing an odometry error.  It could be applicable to a variety of robotic exploration scenarios; for instance, search-and-rescue operations.

In order to focus on the main problem, we abstracted away certain details in our simulation.  In particular, we assume that when a robot senses a known landmark, it can identify it immediately and without error.  Normally there would be some uncertainty involved in, say, a vision sensor identifying visual features, but for simplicity we did not consider this in our model.

In addition, our motion and sensor models are simplified as described in the next section.

\section{Implementation}

We assume a simple linear system with gaussian transition and sensor noise.  Covariance matrices express relationships between landmarks and robot positions and between landmarks themselves.  

We utilized Python and NumPy and for our numerical calculations and the Tkinter graphical framework to visualize our simulation.

Figure~\ref{fig:simulation} shows a screenshot of the simulation.  The black circles represent unique landmarks.  The blue and red circles around or inside landmarks are the robots' estimations of where the landmarks are; the size of the circle represents its uncertainty about the landark's exact position, with a larger circle representing more uncertainty.

\begin{figure}[h!]
\includegraphics[width=\textwidth]{screenshot.png}
\caption{Simulation environment.}
\label{fig:simulation}
\end{figure}

The blue and red circles on the robots themselves represent the robots' estimation and uncertainty about their own position.

A diagram of the system is shown Figure~\ref{fig:system}.

\begin{figure}[h!]
\includegraphics[width=\textwidth]{systemdiagram.png}
\caption{The flow of the program.}
\label{fig:system}
\end{figure}

\subsection{Parameters}

Since the robot starts out not knowing about any landmarks, and adds new landmarks to its database at every time step, we must recompute all the matrices in our model at every timestep. The system state, $x$ is
$$
x 
= 
\begin{bmatrix}
    x_r \\
    y_r \\
    \theta_r \\
    x_{a_1} \\
    y_{a_1} \\
    \vdots \\
    x_{a_n} \\
    y_{a_n} \\
\end{bmatrix}
$$

Where $[x_r, y_r, \theta_r]^T$ is the robot's pose, and $[x_i, y_i]^T$ is landmarks $i$'s $x$ and $y$ location, and $\{a_1,\ldots,a_n\}$ is the ordering of the landmarks in the $x$ vector. This ordering is dependent on the order in which landmarks are discovered by the robot. All locations are in the global reference frame. The estimate of $x$ may be referred to as $\hat{x}$.

P is the covariance of the state estimate. $\hat{x}$ and $P$ are the parameters of the multivariate distribution that describes the current estimate of the system state.

$$
P =
\begin{bmatrix}
    var(x_r)     & 0            & 0               & cov(x_r,x_1) & 0            & cov(x_r,x_2) & 0            \\
    0            & var(y_r)     & 0               & 0            & cov(y_r,y_1) & 0            & cov(y_r,y_2) \\
    0            & 0            & var(\theta)     & 0            & 0            & 0            & 0            \\
    cov(x_r,x_1) & 0            & 0               & var(x_1)     & 0            & cov(x_1,x_2) & 0            \\
    0            & cov(y_r,y_1) & 0               & 0            & var(y_1)     & 0            & cov(y_1,y_2) \\
    cov(x_r,x_2) & 0            & 0               & cov(x_1,x_2) & 0            & var(x_2)     & 0            \\
    0            & cov(y_r,y_2) & 0               & 0            & cov(y_1,y_2) & 0            & var(y_2)     \\
\end{bmatrix}
$$

The diagonals of this matrix are just the variances of each variable in the state vector. The off-diagonal elements are the covariances between different variables in the vector. We choose our $Q$ and $R$ matrices, which are motion covariance and measurement covariance, respectively, to indicate that there is noise between 

\subsection{Motion Model}

We also have the motion command, where $u=[\Delta x_r, \Delta y_r, \Delta \theta_r]^T$, or the change robot pose from timestep $t$ to timestep $t+1$.

For our transition model, we set $F$ to be the identity matrix of the appropriate size. Thus our transition model becomes $x' = x + Gu$. An example of our transition model for one timestep follows. In this example the robot has already observed landmark 1, and has an estimate of that landmark's position in its state: $x_t=[3, 4, \pi, x_1, y_1]^T$, and $u_t = [1,1,0]^T$. In this case the transition model equation will look like this:

$$
\begin{bmatrix}
    4  \\
    5  \\
    \pi\\
    30 \\
    40 \\
\end{bmatrix}
=
\begin{bmatrix}
    3  \\
    4  \\
    \pi\\
    30 \\
    40 \\
\end{bmatrix}
+
\begin{bmatrix}
    1 & 0 & 0  \\
    0 & 1 & 0  \\
    0 & 0 & 1  \\
    0 & 0 & 0  \\
    0 & 0 & 0  \\
\end{bmatrix}
\begin{bmatrix}
    1 \\
    1 \\
    0 \\
\end{bmatrix}
$$

The purpose of $G$ in this model is simply to convert $u_t$ into an $n\times 1$ matrix, where $n$ is the size of $x$, so the change in pose is added to the proper elements in $x$.

\subsection{Sensor Model}

Since our measurements are relative position between the motors and the landmarks, the $H$ matrix in the measurement model must express that. The relative sensor measurement is
$$
z = 
\begin{bmatrix}
    x_{a_1} - x_r \\
    y_{a_1} - y_r \\
    \vdots\\
    x_{a_m} - x_r \\
    y_{a_m} - y_r \\
\end{bmatrix}
$$ 

where $\{a_1,\ldots,a_m\}$ is the list of landmark ids that were measured at timestep $t+1$. An example of our measurement model, if the robot discovered landmarks in the order $\{1,4,2\}$, and has estimates of them in its state, a relative measurement of landmarks 1 and 2 would look like this:

$$
\begin{bmatrix}
    x_1 - x_r \\
    y_1 - y_r \\
    x_2 - x_r \\
    y_2 - y_r \\
\end{bmatrix}
=
\begin{bmatrix}
    -1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
    0 & -1 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
    -1 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 \\
    0 & -1 & 0 & 0 & 0 & 0 & 0 & 0 & 1 \\
\end{bmatrix}
\begin{bmatrix}
    x_r \\
    y_r \\
    \theta_r \\
    x_1 \\
    y_1 \\
    x_4 \\
    y_4 \\
    x_2 \\
    y_2 \\
\end{bmatrix}
$$

This $H$ matrix properly computes the relative measurement from the state's absolute positions.

\subsection{Distributed SLAM}

The most basic approach to distributed SLAM is to assume that communication is always on, and just propagate all measurements and motion commands to every robot. This is simple, but can be computationally expensive, and wastes a lot of compute on each robot combining the same measurements into the covariance matrix.

Our approach is to use another Kalman filter, where the state transition model takes into account the new poses of the robots, and the measurement equation takes into account 

with just an update step and no prediction.  Each robot's state vector is treated as a measurement and the covariance as the measurement noise. The resulting estimates are fed back into each robot at each iteration.

\section{Results}

\begin{thebibliography}{12}
    \bibitem{thrun2005}
        S. Thrun, W. Burgard, and D. Fox, \emph{Probabilistic Robotics}, MIT Press, 2005.

    \bibitem{thrun2003}
        S. Thrun and Y. Liu, ``Multi-Robot SLAM with Sparse Extended Information Filters'', \emph{Proceedings of the 11th International Symposium of Robotics Research (ISRR'03)}, 2003.

    \bibitem{cunningham2010}
        A. Cunningham, M. Paluri, and F. Dellaert, ``DDF-SAM: Fully Distributed SLAM using Constrained Factor Graphs'', \emph{International Conference on Intelligent Robots and Systems (IROS)}, 2010.

\end{thebibliography}

\end{document}
